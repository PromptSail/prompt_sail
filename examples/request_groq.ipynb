{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# How to request to Groq using PromptSail proxy\n",
    "\n",
    "1. Add you API key into ```.env``` file as ```GROQ_API_KEY```, you can generate it [here](https://console.groq.com/keys)\n",
    "2. Add Groq provider to your PromptSail project. Provider structure should look like this one:\n",
    "```json\n",
    "    {\n",
    "        deployment_name: 'Groq',\n",
    "        slug: 'groq',\n",
    "        api_base: 'https://api.groq.com',\n",
    "        description: '',\n",
    "        provider_name: 'Groq'\n",
    "    }\n",
    "```\n",
    "3. Now you can send your request to ```http://localhost:8000/models-playground/groq/openai/v1/chat/completions``` where ```models-playground``` is your project's slug and ```groq``` is the provider's slug and the rest of url is a specific target path necessary for the query to work."
   ],
   "id": "d451cde1277ecc3"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-11T11:40:43.013093Z",
     "start_time": "2024-07-11T11:40:39.126828Z"
    }
   },
   "source": [
    "import requests\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "api_key = config[\"GROQ_API_KEY\"]\n",
    "\n",
    "endpoint = 'http://localhost:8000/models-playground/groq'\n",
    "target_path = '/openai/v1/chat/completions'\n",
    "\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {api_key}',\n",
    "    'Content-Type': 'application/json',\n",
    "}\n",
    "\n",
    "data = {\n",
    "    'messages': [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    'model': 'llama3-8b-8192'\n",
    "}\n",
    "\n",
    "response = requests.post(endpoint + target_path, headers=headers, json=data)\n",
    "if response.status_code == 200:\n",
    "    print('Success:', response.json())\n",
    "else:\n",
    "    print('Error:', response.status_code, response.text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: {'id': 'chatcmpl-59e5cf2e-a474-44dd-967d-f2b155386e73', 'object': 'chat.completion', 'created': 1720698042, 'model': 'llama3-8b-8192', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Fast language models, also known as efficient language models, have become increasingly important in recent years due to the growing demands of natural language processing (NLP) applications. Here are some reasons why fast language models are crucial:\\n\\n1. **Scalability**: As NLP applications continue to grow in complexity and size, fast language models enable them to process large volumes of data quickly and efficiently. This is particularly important for applications that require real-time processing, such as chatbots, voice assistants, and language translation systems.\\n2. **Computational resources**: Fast language models can run on less powerful hardware, reducing the need for expensive computing resources. This makes them suitable for deployment on mobile devices, embedded systems, and other resource-constrained environments.\\n3. **Improved user experience**: Fast language models can respond quickly to user input, providing a better user experience in applications such as messaging, search engines, and dialogue systems.\\n4. **Competitive advantage**: The ability to process language quickly and efficiently can be a key differentiator in competitive markets, such as the development of AI-powered language translation systems.\\n5. **Data processing**: Fast language models can handle large datasets quickly, enabling researchers and developers to process and analyze vast amounts of text data, which is essential for many NLP applications, such as sentiment analysis and text classification.\\n6. **Model inference**: Fast language models can be used to accelerate model inference, reducing the latency and increasing the speed of predictions in applications such as text classification, sentiment analysis, and language translation.\\n7. ** Embedding lookups**: Fast language models can quickly retrieve and process word embeddings, which are essential for many NLP applications, such as text classification, sentiment analysis, and language translation.\\n8. **Inference in edge devices**: Fast language models can run on edge devices, such as smartwatches, smartphones, or IoT devices, enabling real-time language processing in these devices.\\n9. **Improved robustness**: Fast language models can be designed to be more robust to noise and errors, which is essential for applications that require high accuracy, such as speech recognition and language translation.\\n10. **Advancements in AI**: The development of fast language models has the potential to drive advancements in AI, as they can be used to improve the performance of many AI models and applications.\\n\\nSome popular architectures for fast language models include:\\n\\n1. TinyBERT: A compact version of the BERT language model, designed to require significantly less computational resources.\\n2. Distilled BERT: A smaller version of the BERT language model, distilled from the original BERT model to require fewer parameters and less computational resources.\\n3. MobileBERT: A language model specifically designed for mobile devices, which uses a smaller architecture and fewer parameters to reduce computational requirements.\\n4. Longformer: A language model designed for long-range dependencies, which uses a reduced architecture and attention mechanism to reduce computational requirements.\\n\\nThese are just a few examples of the many fast language models available, and their importance is likely to continue to grow as NLP applications become increasingly prominent in various industries.'}, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 18, 'prompt_time': 0.00334495, 'completion_tokens': 617, 'completion_time': 0.493200122, 'total_tokens': 635, 'total_time': 0.49654507200000003}, 'system_fingerprint': 'fp_873a560973', 'x_groq': {'id': 'req_01j2gsgxadf32br8q77fmb4z6h'}}\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
