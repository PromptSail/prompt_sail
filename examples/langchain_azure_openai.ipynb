{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to connect to Azure OpenAI service via PromptSail and Langchain SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import the OpenAI Python SDK and load your API key from the environment.\n",
    "\n",
    "\n",
    "**You will have create Azure OpenAI deployment and get the API key from the Azure portal.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure OpenAI api key=9b2...d67b3\n",
      "Azure OpenAI api endpoint=https://openai-pr...\n",
      "Azure OpenAI deployment name=gpt-35T...\n",
      "Azure OpenAI api version=2023-07-01-preview\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "\n",
    "azure_oai_key = config[\"AZURE_OPENAI_API_KEY\"]\n",
    "api_base_url = config[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "\n",
    "deployment_name = config[\"AZURE_OPENAI_DEPLOYMENT_NAME\"]\n",
    "api_version = config[\"AZURE_OPENAI_API_VERSION\"]\n",
    "\n",
    "print(\n",
    "    f\"Azure OpenAI api key={azure_oai_key[0:3]}...{azure_oai_key[-5:]}\"\n",
    ")\n",
    "print(\n",
    "    f\"Azure OpenAI api endpoint={api_base_url[0:17]}...\"\n",
    ")\n",
    "print(f\"Azure OpenAI deployment name={deployment_name[0:7]}...\")\n",
    "print(f\"Azure OpenAI api version={api_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the direct connection to Azure OpenAI and Langchain SDK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "#from langchain_community.chat_models import AzureChatOpenAI\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful assistant that help rewirte an jira ticket.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Give meaningful title to this bug, RuntimeError: CUDA out of memory. Tried to allocate X MiB (GPU X; X GiB total capacity; X GiB already allocated; X MiB free; X cached)\"\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat = AzureChatOpenAI(\n",
    "    openai_api_key=azure_oai_key,\n",
    "    deployment_name=deployment_name,\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=api_base_url,\n",
    "    #base_url=api_base_url,\n",
    "    #model=deployment_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Bug: RuntimeError: Insufficient CUDA Memory Allocation (GPU X; X GiB total capacity; X GiB already allocated; X MiB free; X cached)')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a request to the AzureOpenAI via PromptSail proxy\n",
    "\n",
    "Run the docker and go to PromptSail UI http://localhost/\n",
    "\n",
    "Create new project with you `project_slug`or edit existing one.\n",
    "\n",
    "Add your own Azure OpenAI provider by editing the project settings, this will map the Azure OpenAI endpoint to new proxy prompt sail URL. \n",
    "\n",
    "Set the `api base url` to your Azure OpenAI endpoint like\n",
    " \n",
    "'https://**azure_openai_resource**.openai.azure.com/'\n",
    " \n",
    " and add meaningfull `deployment name`.\n",
    "\n",
    "\n",
    "In mongo it will create new entry in `ai_providers` array, similar to this one\n",
    "\n",
    "```bash\n",
    "{\n",
    "     ai_providers: [\n",
    "        {\n",
    "            deployment_name: 'azure US deployment',\n",
    "            slug: 'azure-us-deployment',\n",
    "            api_base: 'https://[azure_openai_resource].openai.azure.com/',\n",
    "            description: '',\n",
    "            provider_name: 'Azure OpenAI'\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "```\n",
    "\n",
    "In this case we will use the default `project 2` settings:\n",
    "* with project_slug -> 'project2' \n",
    "* deployment_name -> 'azure-us-deployment'\n",
    "resulting in promptsail proxy url like this: \n",
    "\n",
    "**\"http://localhost:8000/project2/azure-us-deployment\"**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_api_base = \"http://localhost:8000/project2/azure-us-deployment\"\n",
    "\n",
    "ps_api_base = \"http://localhost:8000/edu-project/ps-us2\"\n",
    "\n",
    "\n",
    "chat = AzureChatOpenAI(\n",
    "    openai_api_key=azure_oai_key,\n",
    "    deployment_name=deployment_name,\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=ps_api_base,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Bug: CUDA Out of Memory Error during Memory Allocation\\n\\nDescription:\\nA runtime error is encountered when attempting to allocate memory in the CUDA framework. The error message states that X MiB of memory was attempted to be allocated, but the GPU X has reached its capacity. Of the X GiB total capacity, X GiB has already been allocated, leaving X MiB free. Additionally, there is X MiB of memory cached.\\n\\nSteps to Reproduce:\\n1. Perform actions that require GPU memory allocation.\\n2. Observe the CUDA Out of Memory error.\\n\\nExpected Result:\\nMemory allocation should be successful without encountering any runtime errors.\\n\\nActual Result:\\nA RuntimeError occurs, indicating that the GPU has run out of memory during the memory allocation process.\\n\\nAdditional Information:\\n- GPU Details: [Specify GPU details here]\\n- Total GPU Capacity: [Specify total GPU capacity here]\\n- Memory Already Allocated: [Specify amount of memory already allocated here]\\n- Free Memory: [Specify amount of free memory here]\\n- Cached Memory: [Specify amount of cached memory here]')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt-sail-hDSOLtZB-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
