{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to connect to Azure OpenAI service via PromptSail and Langchain SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First you need to get or create an Azure OpenAI resource. \n",
    "You can do it here: https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/CognitiveServicesHub/~/OpenAI\n",
    "\n",
    "Click on the chosen resource and than the \"Develop\" button in the middle of the screen.\n",
    "\n",
    "Copy the API key and endpoint and paste it into `.env` file in the `examples` folder. \n",
    "\n",
    "Install all the necessary packages from [examples/pyproject.toml](pyproject.toml) by running the following command:\n",
    "\n",
    "```bash \n",
    "cd prompt_sail/examples\n",
    "poetry install\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the connection without PromptSail\n",
    "\n",
    "Import the OpenAI Python SDK and load your API key from the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Azure OpenAI api <span style=\"color: #808000; text-decoration-color: #808000\">key</span>=<span style=\"color: #800080; text-decoration-color: #800080\">9b2</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>d67b3\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Azure OpenAI api \u001B[33mkey\u001B[0m=\u001B[35m9b2\u001B[0m\u001B[33m...\u001B[0md67b3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Azure OpenAI api <span style=\"color: #808000; text-decoration-color: #808000\">endpoint</span>=<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://openai-pr...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Azure OpenAI api \u001B[33mendpoint\u001B[0m=\u001B[4;94mhttps\u001B[0m\u001B[4;94m://openai-pr...\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Azure OpenAI deployment <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #800080; text-decoration-color: #800080\">gpt</span>-35T<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Azure OpenAI deployment \u001B[33mname\u001B[0m=\u001B[35mgpt\u001B[0m-35T\u001B[33m...\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Azure OpenAI api <span style=\"color: #808000; text-decoration-color: #808000\">version</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">07</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">01</span>-preview\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Azure OpenAI api \u001B[33mversion\u001B[0m=\u001B[1;36m2023\u001B[0m-\u001B[1;36m07\u001B[0m-\u001B[1;36m01\u001B[0m-preview\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "from rich import print  \n",
    "\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "\n",
    "azure_oai_key = config[\"AZURE_OPENAI_API_KEY\"]\n",
    "api_base_url = config[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "\n",
    "deployment_name = config[\"AZURE_LLM_DEPLOYMENT_NAME\"]\n",
    "api_version = config[\"AZURE_OPENAI_API_VERSION\"]\n",
    "\n",
    "print(\n",
    "    f\"Azure OpenAI api key={azure_oai_key[0:3]}...{azure_oai_key[-5:]}\"\n",
    ")\n",
    "print(\n",
    "    f\"Azure OpenAI api endpoint={api_base_url[0:17]}...\"\n",
    ")\n",
    "print(f\"Azure OpenAI deployment name={deployment_name[0:7]}...\")\n",
    "print(f\"Azure OpenAI api version={api_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the direct connection to Azure OpenAI and Langchain SDK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai  import AzureChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ChatMessage\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "message = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful assistant that help rewirte an jira ticket.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Give meaningful title to this bug, RuntimeError: CUDA out of memory. Tried to allocate X MiB (GPU X; X GiB total capacity; X GiB already allocated; X MiB free; X cached)\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "message_followup = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful assistant that help rewirte an jira ticket.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Give meaningful title to this bug, RuntimeError: CUDA out of memory. Tried to allocate X MiB (GPU X; X GiB total capacity; X GiB already allocated; X MiB free; X cached)\"\n",
    "    ),\n",
    "    AIMessage(\n",
    "        content=\"RuntimeError: CUDA out of memory. Tried to allocate X MiB (GPU X; X GiB total capacity; X GiB already allocated; X MiB free; X cached)\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"The error message is too technical, could you make it more user friendly and poetic?\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat = AzureChatOpenAI(\n",
    "    openai_api_key=azure_oai_key,\n",
    "    deployment_name=deployment_name,\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=api_base_url,\n",
    "    #base_url=api_base_url,\n",
    "    #model=deployment_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Bug: \"RuntimeError: CUDA out of memory when allocating X MiB (GPU X)\"'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">70</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">89</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gpt-35-turbo'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_filter_results'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_index'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'content_filter_results'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'hate'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'filtered'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'severity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'safe'</span><span style=\"font-weight: bold\">}</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'self_harm'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'filtered'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'severity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'safe'</span><span style=\"font-weight: bold\">}</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'sexual'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'filtered'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'severity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'safe'</span><span style=\"font-weight: bold\">}</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'violence'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'filtered'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'severity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'safe'</span><span style=\"font-weight: bold\">}</span>\n",
       "                <span style=\"font-weight: bold\">}</span>\n",
       "            <span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'content_filter_results'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'hate'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'filtered'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'severity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'safe'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'self_harm'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'filtered'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'severity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'safe'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'sexual'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'filtered'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'severity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'safe'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'violence'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'filtered'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'severity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'safe'</span><span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-e1bcf372-bb08-41f2-a766-1607f1fa13ad-0'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">70</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">89</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;35mAIMessage\u001B[0m\u001B[1m(\u001B[0m\n",
       "    \u001B[33mcontent\u001B[0m=\u001B[32m'Bug: \"RuntimeError: CUDA out of memory when allocating X MiB \u001B[0m\u001B[32m(\u001B[0m\u001B[32mGPU X\u001B[0m\u001B[32m)\u001B[0m\u001B[32m\"'\u001B[0m,\n",
       "    \u001B[33mresponse_metadata\u001B[0m=\u001B[1m{\u001B[0m\n",
       "        \u001B[32m'token_usage'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'completion_tokens'\u001B[0m: \u001B[1;36m19\u001B[0m, \u001B[32m'prompt_tokens'\u001B[0m: \u001B[1;36m70\u001B[0m, \u001B[32m'total_tokens'\u001B[0m: \u001B[1;36m89\u001B[0m\u001B[1m}\u001B[0m,\n",
       "        \u001B[32m'model_name'\u001B[0m: \u001B[32m'gpt-35-turbo'\u001B[0m,\n",
       "        \u001B[32m'system_fingerprint'\u001B[0m: \u001B[3;35mNone\u001B[0m,\n",
       "        \u001B[32m'prompt_filter_results'\u001B[0m: \u001B[1m[\u001B[0m\n",
       "            \u001B[1m{\u001B[0m\n",
       "                \u001B[32m'prompt_index'\u001B[0m: \u001B[1;36m0\u001B[0m,\n",
       "                \u001B[32m'content_filter_results'\u001B[0m: \u001B[1m{\u001B[0m\n",
       "                    \u001B[32m'hate'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'filtered'\u001B[0m: \u001B[3;91mFalse\u001B[0m, \u001B[32m'severity'\u001B[0m: \u001B[32m'safe'\u001B[0m\u001B[1m}\u001B[0m,\n",
       "                    \u001B[32m'self_harm'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'filtered'\u001B[0m: \u001B[3;91mFalse\u001B[0m, \u001B[32m'severity'\u001B[0m: \u001B[32m'safe'\u001B[0m\u001B[1m}\u001B[0m,\n",
       "                    \u001B[32m'sexual'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'filtered'\u001B[0m: \u001B[3;91mFalse\u001B[0m, \u001B[32m'severity'\u001B[0m: \u001B[32m'safe'\u001B[0m\u001B[1m}\u001B[0m,\n",
       "                    \u001B[32m'violence'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'filtered'\u001B[0m: \u001B[3;91mFalse\u001B[0m, \u001B[32m'severity'\u001B[0m: \u001B[32m'safe'\u001B[0m\u001B[1m}\u001B[0m\n",
       "                \u001B[1m}\u001B[0m\n",
       "            \u001B[1m}\u001B[0m\n",
       "        \u001B[1m]\u001B[0m,\n",
       "        \u001B[32m'finish_reason'\u001B[0m: \u001B[32m'stop'\u001B[0m,\n",
       "        \u001B[32m'logprobs'\u001B[0m: \u001B[3;35mNone\u001B[0m,\n",
       "        \u001B[32m'content_filter_results'\u001B[0m: \u001B[1m{\u001B[0m\n",
       "            \u001B[32m'hate'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'filtered'\u001B[0m: \u001B[3;91mFalse\u001B[0m, \u001B[32m'severity'\u001B[0m: \u001B[32m'safe'\u001B[0m\u001B[1m}\u001B[0m,\n",
       "            \u001B[32m'self_harm'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'filtered'\u001B[0m: \u001B[3;91mFalse\u001B[0m, \u001B[32m'severity'\u001B[0m: \u001B[32m'safe'\u001B[0m\u001B[1m}\u001B[0m,\n",
       "            \u001B[32m'sexual'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'filtered'\u001B[0m: \u001B[3;91mFalse\u001B[0m, \u001B[32m'severity'\u001B[0m: \u001B[32m'safe'\u001B[0m\u001B[1m}\u001B[0m,\n",
       "            \u001B[32m'violence'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'filtered'\u001B[0m: \u001B[3;91mFalse\u001B[0m, \u001B[32m'severity'\u001B[0m: \u001B[32m'safe'\u001B[0m\u001B[1m}\u001B[0m\n",
       "        \u001B[1m}\u001B[0m\n",
       "    \u001B[1m}\u001B[0m,\n",
       "    \u001B[33mid\u001B[0m=\u001B[32m'run-e1bcf372-bb08-41f2-a766-1607f1fa13ad-0'\u001B[0m,\n",
       "    \u001B[33musage_metadata\u001B[0m=\u001B[1m{\u001B[0m\u001B[32m'input_tokens'\u001B[0m: \u001B[1;36m70\u001B[0m, \u001B[32m'output_tokens'\u001B[0m: \u001B[1;36m19\u001B[0m, \u001B[32m'total_tokens'\u001B[0m: \u001B[1;36m89\u001B[0m\u001B[1m}\u001B[0m\n",
       "\u001B[1m)\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resp = chat.invoke(message)\n",
    "print(resp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a request to the AzureOpenAI via PromptSail proxy\n",
    "\n",
    "\n",
    "* Go to demo [Try PromptSail](https://try-promptsail.azurewebsites.net/) and create a new project or use an existing one.\n",
    "* [Run the PromptSail docker images](https://promptsail.com/docs/quick-start-guide/#pull-and-run-the-docker-images-from-ghcr) and go to UI at http://localhost/.\n",
    "We will have to setup a project and add ai provider. \n",
    "\n",
    "Create new project with you `project_slug`or edit existing one.\n",
    "\n",
    "Add your own Azure OpenAI provider by editing the project settings, this will map the Azure OpenAI endpoint to new proxy prompt sail URL. \n",
    "\n",
    "Set the `api base url` to your Azure OpenAI endpoint like\n",
    " \n",
    "'https://**azure_openai_resource**.openai.azure.com/'\n",
    " \n",
    " and add meaningfull `deployment name`.\n",
    "\n",
    "\n",
    "In mongo it will create new entry in `ai_providers` array, similar to this one\n",
    "\n",
    "```bash\n",
    "{\n",
    "     ai_providers: [\n",
    "        {\n",
    "            deployment_name: 'azure US deployment',\n",
    "            slug: 'azure-us-deployment',\n",
    "            api_base: 'https://[azure_openai_resource].openai.azure.com/',\n",
    "            description: '',\n",
    "            provider_name: 'Azure OpenAI'\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "```\n",
    "\n",
    "In this case we will use the default `Client campaign` settings:\n",
    "* with project_slug -> 'client-campaign' \n",
    "* deployment_name -> 'azure-us-deployment'\n",
    "resulting in promptsail proxy url like this: \n",
    "\n",
    "**\"http://localhost:8000/client-campaign/azure-us-deployment/\"**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_api_base = \"http://localhost:8000/client-campaign/azure-prompt-sail-eus2/\"\n",
    "\n",
    "\n",
    "chat = AzureChatOpenAI(\n",
    "    openai_api_key=azure_oai_key,\n",
    "    deployment_name=deployment_name,\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=ps_api_base,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Bug: RuntimeError: Insufficient CUDA memory. Attempted to allocate X MiB (GPU X; X GiB total capacity; X GiB already allocated; X MiB free; X cached)')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = chat.invoke(message)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure OpenAI costs calculation and model versions\n",
    "\n",
    "\n",
    "The Azure OpenAI responses do not include the version of the model used to generate the response, request contains the `deployment_name` and response returns just model famili eg, `gpt-3.5-turbo` and not `gpt-3.5-turbo-1106`, this makes impossible to track and calculate costs accurately. To address this issue, you can pass the model_version parameter to the AzureChatOpenAI class, which will append the version to the model name in the output. This allows for easy differentiation between different versions of the model.\n",
    "\n",
    "\n",
    "Ensure that your deployment supports the chosen model version, as different regions support various models and versions. You can verify the supported models and versions in each region by following this\n",
    "[Azure OpenAI Model summary table and region availability](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#model-summary-table-and-region-availability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ps_api_base_model_version_tag = \"http://localhost:8000/client-campaign/azure-prompt-sail-eus2/?tags=examples,langchain-v0.0.354&target_path=\"\n",
    "\n",
    "#ps_api_base_model_version_tag = \"https://try-promptsail.azurewebsites.net/api/models-playground/azure-eastus/?ai_model_version=gpt-4o-2024-05-13&target_path=\"\n",
    "\n",
    "chat_0613 = AzureChatOpenAI(\n",
    "    openai_api_key=azure_oai_key,\n",
    "    deployment_name=deployment_name,\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=ps_api_base_model_version_tag,\n",
    "    #model_version=\"0613\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotFoundError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[41], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mchat_0613\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessage_followup\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\Users\\krzys\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\promptsail-hDSOLtZB-py3.10\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:642\u001B[0m, in \u001B[0;36mBaseChatModel.__call__\u001B[1;34m(self, messages, stop, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m    635\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[0;32m    636\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    637\u001B[0m     messages: List[BaseMessage],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    640\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    641\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BaseMessage:\n\u001B[1;32m--> 642\u001B[0m     generation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(\n\u001B[0;32m    643\u001B[0m         [messages], stop\u001B[38;5;241m=\u001B[39mstop, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    644\u001B[0m     )\u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    645\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(generation, ChatGeneration):\n\u001B[0;32m    646\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m generation\u001B[38;5;241m.\u001B[39mmessage\n",
      "File \u001B[1;32mc:\\Users\\krzys\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\promptsail-hDSOLtZB-py3.10\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:383\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001B[0m\n\u001B[0;32m    381\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n\u001B[0;32m    382\u001B[0m             run_managers[i]\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[1;32m--> 383\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    384\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    385\u001B[0m     LLMResult(generations\u001B[38;5;241m=\u001B[39m[res\u001B[38;5;241m.\u001B[39mgenerations], llm_output\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39mllm_output)\n\u001B[0;32m    386\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results\n\u001B[0;32m    387\u001B[0m ]\n\u001B[0;32m    388\u001B[0m llm_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_combine_llm_outputs([res\u001B[38;5;241m.\u001B[39mllm_output \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results])\n",
      "File \u001B[1;32mc:\\Users\\krzys\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\promptsail-hDSOLtZB-py3.10\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:373\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001B[0m\n\u001B[0;32m    370\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(messages):\n\u001B[0;32m    371\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    372\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m--> 373\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_with_cache(\n\u001B[0;32m    374\u001B[0m                 m,\n\u001B[0;32m    375\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m    376\u001B[0m                 run_manager\u001B[38;5;241m=\u001B[39mrun_managers[i] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    377\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    378\u001B[0m             )\n\u001B[0;32m    379\u001B[0m         )\n\u001B[0;32m    380\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    381\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[1;32mc:\\Users\\krzys\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\promptsail-hDSOLtZB-py3.10\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:529\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[1;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    525\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    526\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    527\u001B[0m     )\n\u001B[0;32m    528\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported:\n\u001B[1;32m--> 529\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n\u001B[0;32m    530\u001B[0m         messages, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    531\u001B[0m     )\n\u001B[0;32m    532\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    533\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\Users\\krzys\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\promptsail-hDSOLtZB-py3.10\\lib\\site-packages\\langchain_community\\chat_models\\openai.py:435\u001B[0m, in \u001B[0;36mChatOpenAI._generate\u001B[1;34m(self, messages, stop, run_manager, stream, **kwargs)\u001B[0m\n\u001B[0;32m    429\u001B[0m message_dicts, params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_message_dicts(messages, stop)\n\u001B[0;32m    430\u001B[0m params \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    431\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams,\n\u001B[0;32m    432\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m: stream} \u001B[38;5;28;01mif\u001B[39;00m stream \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m {}),\n\u001B[0;32m    433\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    434\u001B[0m }\n\u001B[1;32m--> 435\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompletion_with_retry(\n\u001B[0;32m    436\u001B[0m     messages\u001B[38;5;241m=\u001B[39mmessage_dicts, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\n\u001B[0;32m    437\u001B[0m )\n\u001B[0;32m    438\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_chat_result(response)\n",
      "File \u001B[1;32mc:\\Users\\krzys\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\promptsail-hDSOLtZB-py3.10\\lib\\site-packages\\langchain_community\\chat_models\\openai.py:352\u001B[0m, in \u001B[0;36mChatOpenAI.completion_with_retry\u001B[1;34m(self, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    350\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001B[39;00m\n\u001B[0;32m    351\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_openai_v1():\n\u001B[1;32m--> 352\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mcreate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    354\u001B[0m retry_decorator \u001B[38;5;241m=\u001B[39m _create_retry_decorator(\u001B[38;5;28mself\u001B[39m, run_manager\u001B[38;5;241m=\u001B[39mrun_manager)\n\u001B[0;32m    356\u001B[0m \u001B[38;5;129m@retry_decorator\u001B[39m\n\u001B[0;32m    357\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_completion_with_retry\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
      "File \u001B[1;32mc:\\Users\\krzys\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\promptsail-hDSOLtZB-py3.10\\lib\\site-packages\\openai\\_utils\\_utils.py:272\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    270\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    271\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[1;32m--> 272\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\Users\\krzys\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\promptsail-hDSOLtZB-py3.10\\lib\\site-packages\\openai\\resources\\chat\\completions.py:645\u001B[0m, in \u001B[0;36mCompletions.create\u001B[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[0;32m    596\u001B[0m \u001B[38;5;129m@required_args\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m    597\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[0;32m    598\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    643\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m NOT_GIVEN,\n\u001B[0;32m    644\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatCompletion \u001B[38;5;241m|\u001B[39m Stream[ChatCompletionChunk]:\n\u001B[1;32m--> 645\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    646\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/chat/completions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    647\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    648\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[0;32m    649\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmessages\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    650\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    651\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfrequency_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    652\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunction_call\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunction_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    653\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunctions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunctions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    654\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogit_bias\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    655\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    656\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    657\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    658\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpresence_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    659\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mresponse_format\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    660\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseed\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    661\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstop\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    662\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    663\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtemperature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    664\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtool_choice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    665\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtools\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    666\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_logprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_logprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    667\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_p\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    668\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    669\u001B[0m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    670\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletionCreateParams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    671\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    672\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    673\u001B[0m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[0;32m    674\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    675\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mChatCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    676\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    677\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mChatCompletionChunk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    678\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\Users\\krzys\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\promptsail-hDSOLtZB-py3.10\\lib\\site-packages\\openai\\_base_client.py:1088\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1074\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[0;32m   1075\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1076\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1083\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1084\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m   1085\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[0;32m   1086\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[0;32m   1087\u001B[0m     )\n\u001B[1;32m-> 1088\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32mc:\\Users\\krzys\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\promptsail-hDSOLtZB-py3.10\\lib\\site-packages\\openai\\_base_client.py:853\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m    844\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[0;32m    845\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    846\u001B[0m     cast_to: Type[ResponseT],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    851\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    852\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m--> 853\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    854\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    855\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    856\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    857\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    858\u001B[0m \u001B[43m        \u001B[49m\u001B[43mremaining_retries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mremaining_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    859\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\Users\\krzys\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\promptsail-hDSOLtZB-py3.10\\lib\\site-packages\\openai\\_base_client.py:930\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m    927\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mis_closed:\n\u001B[0;32m    928\u001B[0m         err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m--> 930\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    932\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_response(\n\u001B[0;32m    933\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[0;32m    934\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    937\u001B[0m     stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[0;32m    938\u001B[0m )\n",
      "\u001B[1;31mNotFoundError\u001B[0m: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}"
     ]
    }
   ],
   "source": [
    "chat_0613(message_followup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt-sail-hDSOLtZB-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
