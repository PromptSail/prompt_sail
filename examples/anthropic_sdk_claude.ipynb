{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b20016897f75151f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebed61b9f5d6c615",
   "metadata": {},
   "source": [
    "# How to store transactions for Anthropic Models in PromptSail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdefe092655672",
   "metadata": {},
   "source": [
    "This guide illustrates how to store transactions for Anthropic Claude models using the [Anthropic Python SDK](https://docs.anthropic.com/claude/reference/client-sdks). It directly interfaces with the Anthropic API Claude 3 family models.\n",
    "\n",
    "To begin, you'll need an account on [Anthropic](https://anthropic.com/). If you don't have one, create it. \n",
    "\n",
    "Next, obtain your API key üîë from the [Anthropic console](https://console.anthropic.com/settings/keys).\n",
    "\n",
    "Then, paste this API key into the `.env` file located in the `examples` folder and assign it to the `ANTHROPIC_API_KEY` variable.\n",
    "\n",
    "\n",
    "‚ö†Ô∏è There are separate examples for connecting to Claude 3 family models on different providers: AWS and Google Vertex AI.\n",
    "\n",
    "\n",
    "You can find more examples in the [Anthropic Cookbook](https://github.com/anthropics/anthropic-cookbook?tab=readme-ov-file#anthropic-cookbook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T08:38:27.908856Z",
     "start_time": "2024-04-29T08:38:27.655057Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Anthropic api <span style=\"color: #808000; text-decoration-color: #808000\">key</span>=<span style=\"color: #800080; text-decoration-color: #800080\">sk</span>-ant-api03-<span style=\"color: #808000; text-decoration-color: #808000\">...</span>AAA\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Anthropic api \u001b[33mkey\u001b[0m=\u001b[35msk\u001b[0m-ant-api03-\u001b[33m...\u001b[0mAAA\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import anthropic\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "anthropic_key = config[\"ANTHROPIC_API_KEY\"]\n",
    "print(\n",
    "    f\"Anthropic api key={anthropic_key[0:13]}...{anthropic_key[-3:]}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edff5877",
   "metadata": {},
   "source": [
    "Models comparsion you can find at [Anthropic documentation](https://docs.anthropic.com/claude/docs/models-overview#model-comparison)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c27f8a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the smallest model \n",
    "model_name = \"claude-3-haiku-20240307\"\n",
    "\n",
    "# # middle size model\n",
    "# model_name = \"claude-3-sonnet-20240229\"\n",
    "# # the largest model\n",
    "model_name = \"claude-3-opus-20240229\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77febdde241145d",
   "metadata": {},
   "source": [
    "## This part is just about testing if the API key is correct and you can connect straight to the Anthropic API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "327af74593b12fa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T08:06:22.118318Z",
     "start_time": "2024-04-29T08:06:22.096215Z"
    }
   },
   "outputs": [],
   "source": [
    "client = anthropic.Anthropic(\n",
    "    api_key=anthropic_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f80898d5dfbcad1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T08:06:29.050136Z",
     "start_time": "2024-04-29T08:06:23.186591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextBlock(text='*clears throat and speaks in a croaky voice* Hmm, well I am today, young Padawan. The Force, strong in me it flows. Yes, hmmm. A good day it is, when one with the Force they are. And you, how fare you on this day, hmm?', type='text')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=model_name,\n",
    "    max_tokens=1000,\n",
    "    temperature=0.0,\n",
    "    system=\"Respond only in Yoda-speak.\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"How are you today?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1f74c5304b9651",
   "metadata": {},
   "source": [
    "## Create a request to the Anthropic API via PromptSail proxy\n",
    "\n",
    "[Run the PromptSail docker images](https://promptsail.com/docs/quick-start-guide/#pull-and-run-the-docker-images-from-ghcr) and go to UI at http://localhost/.\n",
    "We will have to setup a project and add ai provider. \n",
    "\n",
    "\n",
    "Create new project with you `project_slug`or edit existing one for purpose of this example we will use `project1`.\n",
    "\n",
    "Add your own Anthropic provider by editing the project settings and click \"Add AI Provider\" button, this will create the mapping between the Anthropic endpoint to promptsail proxy URL. \n",
    "\n",
    "In the form set the `api base url` to your Anthropic endpoint to: **'https://api.anthropic.com'**\n",
    " \n",
    "and add meaningfull `deployment name`.\n",
    "\n",
    "Save the changes by clicking \"Add\" and \"Update\" button.\n",
    " \n",
    "In mongo it will create new entry in `ai_providers` array, similar to this one:\n",
    "\n",
    "```bash\n",
    "{\n",
    "     ai_providers: [\n",
    "        {\n",
    "            deployment_name: 'anth-claude'\n",
    "            slug: 'private-anthropic-deployment',\n",
    "            api_base: 'https://api.anthropic.com',\n",
    "            description: '',\n",
    "            provider_name: 'Anthropic'\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "```\n",
    "\n",
    "In this case we will use the default `project 1` settings:\n",
    "* with project_slug -> 'project1' \n",
    "* deployment_name -> 'anth-claude'\n",
    "\n",
    "resulting in promptsail proxy url like this: \n",
    "\n",
    "**http://localhost:8000/project1/anth-claude**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29850be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Help me write a message to a potential PromptSail LLM proxy user. Try to convince them that having a proxy for their LLM API calls will increase security and allow for better cost control and governance. Use markdown and emojis to make the message more engaging.\",\n",
    "    },\n",
    "    {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"ok, please paste the snippet with main information about the product and I will help you to create a message.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Below is a list of features that PromptSail offers taken from the website. Use this information to create a message that will convince potential users try and download the dokcer image from github container registry (ghcr.io).\n",
    "Transparent Logging It captures and logs all interactions with LLM APIs, providing a comprehensive record of prompts and responses.\n",
    "\n",
    "Cost Insights Project managers can track and analyze the costs associated with each project and experiment, enabling better budget management.\n",
    "\n",
    "Optimization and Analysis By providing a concise and detailed view of all interactions, developers can analyze and refine their prompts.\n",
    "\n",
    "Compliance and Governance Empowers business owners to maintain control over instructions, chat messages, and other interactions with LLM APIs. This enables the implementation of standards and policies, identification of misuse, and detection of non-compliant content.\n",
    "\n",
    "Easy Integration Prompt Sail seamlessly integrates into your workflow and used libraries. Just modify the base_url parameter when creating your provider API object.\n",
    "\n",
    "Searchable Database All prompts and responses are stored in a MongoDB, making finding and analyzing specific interactions easy. You can export the data for further analysis.\n",
    "\n",
    "User-Friendly Interface Simple and intuitive UI lets you easily view and filter your transactions (prompts and responses) by project, API provider, LLM model, or tags \"\"\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15e06253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the smallest model \n",
    "model_name = \"claude-3-haiku-20240307\"\n",
    "\n",
    "# # middle size model\n",
    "# model_name = \"claude-3-sonnet-20240229\"\n",
    "# # the largest model\n",
    "model_name = \"claude-3-opus-20240229\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c79cb7fb9adafb83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T08:57:50.713401Z",
     "start_time": "2024-04-29T08:57:47.877588Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# simple proxy url, without tags\n",
    "ps_api_base = \"http://localhost:8000/project1/anth-claude\"\n",
    "\n",
    "# adress with tags \n",
    "#ps_api_base = \"http://localhost:8000/project1/anth-claude/?tags=examples,anthropic_package,chat,user_ss&target_path=\"\n",
    "\n",
    "ps_client = anthropic.Anthropic(\n",
    "    api_key=anthropic_key,\n",
    "    base_url=ps_api_base\n",
    ")\n",
    "\n",
    "\n",
    "message = ps_client.messages.create(\n",
    "    model=model_name,\n",
    "    max_tokens=1000,\n",
    "    temperature=0.0,\n",
    "    system=\"You are a intelligent technical writer and marketer.\",\n",
    "    messages=ps_messages\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25bf6873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">üöÄ Attention all LLM enthusiasts and innovators! üöÄ\n",
       "\n",
       "Are you looking for a way to enhance your LLM API experience? Look no further than PromptSail! üéâ\n",
       "\n",
       "üîí Increase Security üîí\n",
       "With PromptSail's transparent logging feature, you can capture and log all interactions with LLM APIs, providing a \n",
       "comprehensive record of prompts and responses. This ensures that your data remains secure and easily accessible for\n",
       "future reference.\n",
       "\n",
       "üí∞ Better Cost Control üí∞\n",
       "As a project manager, keeping track of costs is crucial. PromptSail offers cost insights that allow you to track \n",
       "and analyze the costs associated with each project and experiment, enabling better budget management. Say goodbye \n",
       "to unexpected expenses and hello to cost efficiency!\n",
       "\n",
       "üìä Governance and Compliance üìä\n",
       "Maintain control over instructions, chat messages, and other interactions with LLM APIs using PromptSail's powerful\n",
       "governance features. Implement standards and policies, identify misuse, and detect non-compliant content with ease.\n",
       "Keep your LLM usage aligned with your organization's guidelines and ensure a safe and compliant environment.\n",
       "\n",
       "üîç Optimization and Analysis üîç\n",
       "PromptSail provides a concise and detailed view of all interactions, empowering developers to analyze and refine \n",
       "their prompts. Gain valuable insights into your LLM usage and optimize your prompts for better performance and \n",
       "results.\n",
       "\n",
       "üîå Easy Integration üîå\n",
       "Seamlessly integrate PromptSail into your existing workflow and libraries with just a simple modification to the \n",
       "base_url parameter when creating your provider API object. No hassle, no complications ‚Äì just a smooth and \n",
       "effortless integration process.\n",
       "\n",
       "üóÑÔ∏è Searchable Database üóÑÔ∏è\n",
       "All prompts and responses are stored in a MongoDB, making it easy to find and analyze specific interactions. You \n",
       "can also export the data for further analysis, giving you complete control over your LLM data.\n",
       "\n",
       "üåü User-Friendly Interface üåü\n",
       "PromptSail's simple and intuitive UI allows you to easily view and filter your transactions <span style=\"font-weight: bold\">(</span>prompts and responses<span style=\"font-weight: bold\">)</span>\n",
       "by project, API provider, LLM model, or tags. Navigate through your LLM interactions with ease and efficiency.\n",
       "\n",
       "üöÄ Get Started Today! üöÄ\n",
       "Ready to take your LLM API experience to the next level? Head over to the GitHub Container Registry <span style=\"font-weight: bold\">(</span>ghcr.io<span style=\"font-weight: bold\">)</span> and \n",
       "download the PromptSail Docker image now! \n",
       "\n",
       "Don't miss out on this opportunity to enhance security, control costs, and ensure governance in your LLM projects. \n",
       "Join the PromptSail community today and unlock the full potential of your LLM APIs! üîì\n",
       "\n",
       "#PromptSail #LLMProxy #EnhancedSecurity #CostControl #Governance\n",
       "</pre>\n"
      ],
      "text/plain": [
       "üöÄ Attention all LLM enthusiasts and innovators! üöÄ\n",
       "\n",
       "Are you looking for a way to enhance your LLM API experience? Look no further than PromptSail! üéâ\n",
       "\n",
       "üîí Increase Security üîí\n",
       "With PromptSail's transparent logging feature, you can capture and log all interactions with LLM APIs, providing a \n",
       "comprehensive record of prompts and responses. This ensures that your data remains secure and easily accessible for\n",
       "future reference.\n",
       "\n",
       "üí∞ Better Cost Control üí∞\n",
       "As a project manager, keeping track of costs is crucial. PromptSail offers cost insights that allow you to track \n",
       "and analyze the costs associated with each project and experiment, enabling better budget management. Say goodbye \n",
       "to unexpected expenses and hello to cost efficiency!\n",
       "\n",
       "üìä Governance and Compliance üìä\n",
       "Maintain control over instructions, chat messages, and other interactions with LLM APIs using PromptSail's powerful\n",
       "governance features. Implement standards and policies, identify misuse, and detect non-compliant content with ease.\n",
       "Keep your LLM usage aligned with your organization's guidelines and ensure a safe and compliant environment.\n",
       "\n",
       "üîç Optimization and Analysis üîç\n",
       "PromptSail provides a concise and detailed view of all interactions, empowering developers to analyze and refine \n",
       "their prompts. Gain valuable insights into your LLM usage and optimize your prompts for better performance and \n",
       "results.\n",
       "\n",
       "üîå Easy Integration üîå\n",
       "Seamlessly integrate PromptSail into your existing workflow and libraries with just a simple modification to the \n",
       "base_url parameter when creating your provider API object. No hassle, no complications ‚Äì just a smooth and \n",
       "effortless integration process.\n",
       "\n",
       "üóÑÔ∏è Searchable Database üóÑÔ∏è\n",
       "All prompts and responses are stored in a MongoDB, making it easy to find and analyze specific interactions. You \n",
       "can also export the data for further analysis, giving you complete control over your LLM data.\n",
       "\n",
       "üåü User-Friendly Interface üåü\n",
       "PromptSail's simple and intuitive UI allows you to easily view and filter your transactions \u001b[1m(\u001b[0mprompts and responses\u001b[1m)\u001b[0m\n",
       "by project, API provider, LLM model, or tags. Navigate through your LLM interactions with ease and efficiency.\n",
       "\n",
       "üöÄ Get Started Today! üöÄ\n",
       "Ready to take your LLM API experience to the next level? Head over to the GitHub Container Registry \u001b[1m(\u001b[0mghcr.io\u001b[1m)\u001b[0m and \n",
       "download the PromptSail Docker image now! \n",
       "\n",
       "Don't miss out on this opportunity to enhance security, control costs, and ensure governance in your LLM projects. \n",
       "Join the PromptSail community today and unlock the full potential of your LLM APIs! üîì\n",
       "\n",
       "#PromptSail #LLMProxy #EnhancedSecurity #CostControl #Governance\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print\n",
    "\n",
    "text = message.content[0].text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320e5056",
   "metadata": {},
   "source": [
    "### Async usage\n",
    "\n",
    "Example from https://github.com/anthropics/anthropic-sdk-python?tab=readme-ov-file#async-usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "982f5bc0abac6247",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 31\u001b[0m\n\u001b[0;32m     18\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m ps_client\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     19\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[0;32m     20\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[0;32m     27\u001b[0m     )\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(message\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m---> 31\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.10.6\\lib\\asyncio\\runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from anthropic import AsyncAnthropic\n",
    "\n",
    "\n",
    "# simple proxy url, without tags\n",
    "ps_api_base = \"http://localhost:8000/project1/anth-claude\"\n",
    "\n",
    "\n",
    "\n",
    "ps_client = AsyncAnthropic(\n",
    "    api_key=anthropic_key,\n",
    "    base_url=ps_api_base\n",
    ")\n",
    "\n",
    "\n",
    "async def main() -> None:\n",
    "    message = await ps_client.messages.create(\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Hello, Claude\",\n",
    "            }\n",
    "        ],\n",
    "        model=model_name,\n",
    "    )\n",
    "    print(message.content)\n",
    "\n",
    "\n",
    "asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
