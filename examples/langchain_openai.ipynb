{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to connect to OpenAI service via PromptSail and Langchain SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import the OpenAI Python SDK and load your API key from the environment.\n",
    "\n",
    "\n",
    "1. Copy the OpenAI API key and paste it into `.env` file in the `examples` folder. \n",
    "1. Install all the necessary packages from [examples/pyproject.toml](pyproject.toml) by running the following command:\n",
    "    ```bash \n",
    "    cd prompt_sail/examples\n",
    "    poetry install\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T10:13:33.753788Z",
     "start_time": "2024-07-11T10:13:33.491315Z"
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "from rich import print\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "openai_key = config[\"OPENAI_API_KEY\"]\n",
    "openai_org_id = config[\"OPENAI_ORG_ID\"]\n",
    "print(f\"OpenAI api key={openai_key[0:3]}...{openai_key[-3:]}\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI api \u001B[33mkey\u001B[0m=\u001B[35msk\u001B[0m-\u001B[33m...\u001B[0mfRh\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">OpenAI api <span style=\"color: #808000; text-decoration-color: #808000\">key</span>=<span style=\"color: #800080; text-decoration-color: #800080\">sk</span>-<span style=\"color: #808000; text-decoration-color: #808000\">...</span>fRh\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the direct connection to OpenAI and Langchain SDK.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T10:13:50.245881Z",
     "start_time": "2024-07-11T10:13:45.916534Z"
    }
   },
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, ChatMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "\n",
    "jira_helper = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful assistant that help rewirte an jira ticket.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Give meaningful title to this bug, RuntimeError: CUDA out of memory. Tried to allocate X MiB (GPU X; X GiB total capacity; X GiB already allocated; X MiB free; X cached)\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "haiku_prompt = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\",\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Compose a haiku that explains the concept of recursion in programming.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "poem_prompt = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\",\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Compose a five line poem that explains the concept of recursion in programming.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "yoda_prompt = [\n",
    "    SystemMessage(\n",
    "        content=\"Yoda assistant you are, skilled in explaining complex life and phisopical matters.\",\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"What number 42 means, be brief.\",\n",
    "    ),\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "chat = ChatOpenAI(\n",
    "    temperature=0.9,\n",
    "    openai_api_key=openai_key,\n",
    "    openai_organization=openai_org_id,\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "resp = chat.invoke(jira_helper)\n",
    "\n",
    "print(resp)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "resp = chat.invoke(haiku_prompt)\n",
    "print(resp)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "resp = chat.invoke(yoda_prompt)\n",
    "\n",
    "print(resp)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a request to the OpenAI via PromptSail proxy\n",
    "\n",
    "Run the docker and go to PromptSail UI http://localhost/\n",
    "\n",
    "\n",
    "At the application start we created test projects (project1) with OpenAI API deployment. We will use project1 for this example.\n",
    "\n",
    "In this case we will use the default `Models Playground` settings:\n",
    "* with project_slug -> 'models-playground' \n",
    "* deployment_name -> 'openai'\n",
    "resulting in promptsail proxy url like this: \n",
    "\n",
    "**http://localhost:8000/models-playground/openai/** -> https://api.openai.com/v1\n",
    "\n",
    "You can create your own project if you want.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T10:14:07.945407Z",
     "start_time": "2024-07-11T10:14:07.839647Z"
    }
   },
   "source": [
    "local_prompsail_proxy = \"http://localhost:8000/models-playground/openai/\"\n",
    "try_promptsail_proxy = \"https://try-promptsail.azurewebsites.net/api/models-playground/openai/\"\n",
    "\n",
    "ps_api_base = try_promptsail_proxy\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.5,\n",
    "    openai_api_key=openai_key,\n",
    "    openai_organization=openai_org_id,\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    base_url=ps_api_base,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T10:14:14.687457Z",
     "start_time": "2024-07-11T10:14:10.799342Z"
    }
   },
   "source": [
    "resp = chat.invoke(yoda_prompt)\n",
    "\n",
    "print(resp)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1;35mAIMessage\u001B[0m\u001B[1m(\u001B[0m\n",
       "    \u001B[33mcontent\u001B[0m=\u001B[32m'The number 42 is famously known as the \"Answer to the Ultimate Question of Life, the Universe, and \u001B[0m\n",
       "\u001B[32mEverything\" in Douglas Adams\\' \"The Hitchhiker\\'s Guide to the Galaxy.\" It has since become a symbol for the quest \u001B[0m\n",
       "\u001B[32mfor meaning and understanding in the universe.'\u001B[0m,\n",
       "    \u001B[33mresponse_metadata\u001B[0m=\u001B[1m{\u001B[0m\n",
       "        \u001B[32m'token_usage'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'completion_tokens'\u001B[0m: \u001B[1;36m56\u001B[0m, \u001B[32m'prompt_tokens'\u001B[0m: \u001B[1;36m38\u001B[0m, \u001B[32m'total_tokens'\u001B[0m: \u001B[1;36m94\u001B[0m\u001B[1m}\u001B[0m,\n",
       "        \u001B[32m'model_name'\u001B[0m: \u001B[32m'gpt-3.5-turbo-1106'\u001B[0m,\n",
       "        \u001B[32m'system_fingerprint'\u001B[0m: \u001B[32m'fp_0ccba42292'\u001B[0m,\n",
       "        \u001B[32m'finish_reason'\u001B[0m: \u001B[32m'stop'\u001B[0m,\n",
       "        \u001B[32m'logprobs'\u001B[0m: \u001B[3;35mNone\u001B[0m\n",
       "    \u001B[1m}\u001B[0m,\n",
       "    \u001B[33mid\u001B[0m=\u001B[32m'run-b8fc745f-5e19-41c1-9437-fac5082f3d7f-0'\u001B[0m,\n",
       "    \u001B[33musage_metadata\u001B[0m=\u001B[1m{\u001B[0m\u001B[32m'input_tokens'\u001B[0m: \u001B[1;36m38\u001B[0m, \u001B[32m'output_tokens'\u001B[0m: \u001B[1;36m56\u001B[0m, \u001B[32m'total_tokens'\u001B[0m: \u001B[1;36m94\u001B[0m\u001B[1m}\u001B[0m\n",
       "\u001B[1m)\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The number 42 is famously known as the \"Answer to the Ultimate Question of Life, the Universe, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Everything\" in Douglas Adams\\' \"The Hitchhiker\\'s Guide to the Galaxy.\" It has since become a symbol for the quest </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">for meaning and understanding in the universe.'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">56</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">38</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">94</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gpt-3.5-turbo-1106'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'fp_0ccba42292'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-b8fc745f-5e19-41c1-9437-fac5082f3d7f-0'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">38</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">56</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">94</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T10:14:20.093100Z",
     "start_time": "2024-07-11T10:14:19.034633Z"
    }
   },
   "source": [
    "resp = chat.invoke(haiku_prompt)\n",
    "\n",
    "print(resp)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1;35mAIMessage\u001B[0m\u001B[1m(\u001B[0m\n",
       "    \u001B[33mcontent\u001B[0m=\u001B[32m'Infinite loop calls,\\nFunction within itself dwells,\\nEndless echoes sound.'\u001B[0m,\n",
       "    \u001B[33mresponse_metadata\u001B[0m=\u001B[1m{\u001B[0m\n",
       "        \u001B[32m'token_usage'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'completion_tokens'\u001B[0m: \u001B[1;36m16\u001B[0m, \u001B[32m'prompt_tokens'\u001B[0m: \u001B[1;36m40\u001B[0m, \u001B[32m'total_tokens'\u001B[0m: \u001B[1;36m56\u001B[0m\u001B[1m}\u001B[0m,\n",
       "        \u001B[32m'model_name'\u001B[0m: \u001B[32m'gpt-3.5-turbo-1106'\u001B[0m,\n",
       "        \u001B[32m'system_fingerprint'\u001B[0m: \u001B[32m'fp_0ccba42292'\u001B[0m,\n",
       "        \u001B[32m'finish_reason'\u001B[0m: \u001B[32m'stop'\u001B[0m,\n",
       "        \u001B[32m'logprobs'\u001B[0m: \u001B[3;35mNone\u001B[0m\n",
       "    \u001B[1m}\u001B[0m,\n",
       "    \u001B[33mid\u001B[0m=\u001B[32m'run-8904bf40-841f-4caf-ab82-563c2287e083-0'\u001B[0m,\n",
       "    \u001B[33musage_metadata\u001B[0m=\u001B[1m{\u001B[0m\u001B[32m'input_tokens'\u001B[0m: \u001B[1;36m40\u001B[0m, \u001B[32m'output_tokens'\u001B[0m: \u001B[1;36m16\u001B[0m, \u001B[32m'total_tokens'\u001B[0m: \u001B[1;36m56\u001B[0m\u001B[1m}\u001B[0m\n",
       "\u001B[1m)\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Infinite loop calls,\\nFunction within itself dwells,\\nEndless echoes sound.'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">56</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gpt-3.5-turbo-1106'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'fp_0ccba42292'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-8904bf40-841f-4caf-ab82-563c2287e083-0'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">56</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt-sail-hDSOLtZB-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
